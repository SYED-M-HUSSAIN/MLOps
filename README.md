# ğŸ¤– AI-Powered Code Review Automation with Anthropic Claude

> Automate intelligent, consistent, and scalable code reviews directly in your GitHub pull requests using Anthropic's Claude API.

## ğŸ“Œ Overview

This repository implements a modular, CI-integrated code review framework that leverages Anthropic Claude to automatically review pull requests. The system analyzes code diffs, applies human-centric evaluation criteria, and posts structured, actionable feedback as PR comments.

## ğŸ§  Core Objectives

- **Automate** high-quality PR reviews at scale
- **Ensure consistency** using criteria-driven analysis
- **Enhance developer productivity** by reducing review latency
- **Utilize LLMs** to identify bugs, suggest improvements, and uphold coding standards

## ğŸ› ï¸ System Architecture

```
GitHub PR Event â”€â”¬â”€â–¶ GitHub Actions Workflow
                 â”‚
                 â””â”€â–¶ Python App (scripts/)
                      â”œâ”€ Fetch PR diff
                      â”œâ”€ Load evaluation criteria
                      â”œâ”€ Call Anthropic Claude API
                      â”œâ”€ Parse response
                      â””â”€ Post PR comment via GitHub API
```

## ğŸ“ Repository Structure

```
MLOps/
â”œâ”€â”€ scripts/                   # Core logic
â”‚   â”œâ”€â”€ main.py                # Entry point
â”‚   â”œâ”€â”€ reviewer.py            # PR review pipeline
â”‚   â”œâ”€â”€ github_utils.py        # GitHub API interaction
â”‚   â”œâ”€â”€ anthropic_utils.py     # Claude API wrapper
â”‚   â”œâ”€â”€ prompts.py             # Advanced prompt engineering templates
â”‚   â”œâ”€â”€ config.py              # Environment and constants
â”‚   â””â”€â”€ utils.py               # Helpers (e.g., markdown criteria loader)
â”‚
â”œâ”€â”€ evaluation_criteria/       # Human-aligned review principles
â”‚   â”œâ”€â”€ README.md              # Instructions for writing criteria
â”‚   â”œâ”€â”€ code_quality.md
â”‚   â”œâ”€â”€ performance.md
â”‚   â”œâ”€â”€ documentation.md
â”‚   â””â”€â”€ security.md
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ review.yml         # GitHub Actions CI pipeline
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### âœ… Prerequisites

- Python 3.10+
- GitHub Personal Access Token
- Anthropic Claude API Key

### ğŸ“¥ Installation

```bash
git clone https://github.com/SYED-M-HUSSAIN/MLOps.git
cd MLOps
python3 -m venv .venv
source .venv/bin/activate
pip install -e .
```


### â–¶ï¸ Run Locally

```bash
python scripts/main.py
```

## ğŸ“œ Writing Evaluation Criteria

Evaluation files are structured markdown documents under `evaluation_criteria/`, allowing modularity, explainability, and transparency of the review system.

Each file should include sections such as:

```markdown
# Code Quality

### Positive Examples
- Uses meaningful variable names
- Follows single-responsibility principle

### Negative Examples
- Deeply nested conditionals
- Repetitive logic without abstraction
```

These files are parsed and embedded in the system prompt to guide Claude's analysis.

Refer to [`evaluation_criteria/README.md`](evaluation_criteria/README.md) for detailed authoring guidelines.

## ğŸ¤– Prompt Engineering

Prompt templates are constructed in `scripts/prompts.py` using structured prompt engineering best practices:

- **System prompt** defines the reviewer's role and tone.
- **User prompt** includes code diffs and target instructions.
- Prompts are **contextualized** using evaluation criteria.

This separation enables maintainable and testable interactions with the LLM.

## ğŸ” GitHub Actions Integration

The `.github/workflows/review.yml` pipeline triggers the review on PR open or update.

```yaml
on:
  pull_request:
    types: [opened, synchronize]
```

You can extend this pipeline to:
- Run tests on the Claude response
- Notify developers via Slack
- Auto-approve or label PRs

## ğŸ§ª Testing and Validation

**Unit tests** (coming soon) will cover:

- Diff parsing logic
- Prompt formatting
- Claude response validation
- Markdown criteria loading

To run tests:

```bash
pytest tests/
```

## ğŸ” Security Considerations

- **Secrets** such as API keys must be stored in GitHub Secrets or `.env` files.
- **LLM responses** are sandboxed and validated before GitHub posting.
- Avoid logging sensitive data.

## ğŸ”„ Extending the System

You can easily:

- Add more criteria categories
- Support multi-file diffs
- Swap Claude with OpenAI/Gemini via modular prompt layers
- Track response accuracy with custom feedback loops

## ğŸ‘¥ Contributing

We welcome contributions in the form of:

- Improved prompts or criteria
- Support for additional models
- Enhancements to GitHub workflows
- Documentation improvements

Please follow [Semantic Commit Messages](https://www.conventionalcommits.org/) and open a PR with a detailed description.

## ğŸ“ƒ License

This project is released under the MIT License. See `LICENSE` for more information.

## ğŸ”— Resources

- [Anthropic Claude API Docs](https://docs.anthropic.com/)
- [GitHub REST API](https://docs.github.com/en/rest)
- [Best Practices for LLM Apps](https://github.com/openai/openai-cookbook)